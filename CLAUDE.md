# claw-bench - Agent Instructions

## What This Is

This is a benchmark suite for testing clawdbot agents. It runs standardized tests against different LLM models via AWS Bedrock to measure tool use capability, response quality, and reliability.

## Key Rules

1. **Never modify test results manually** - Reports in `reports/` are auto-generated by `benchmark-models.sh`
2. **Keep README scores in sync** - When new benchmarks are run, update the scores table at the top of README.md
3. **Test changes locally first** - Run `./run.sh --local` before committing test changes

## Directory Structure

```
claw-bench/
├── run.sh                 # Main benchmark runner
├── benchmark-models.sh    # Multi-model benchmark script (generates reports)
├── models-to-test.json    # Model configurations for benchmarking
├── tests/                 # Individual test cases (NN_test_name.sh)
├── lib/                   # Shared helper functions
├── reports/               # Auto-generated benchmark reports (DO NOT EDIT)
├── config.example.sh      # Example configuration
└── README.md              # Documentation with scores table
```

## Running Benchmarks

```bash
# Single model, local clawdbot
./run.sh --local

# Single model via SSH
CLAW_HOST="ubuntu@ip" CLAW_SSH_KEY="~/.ssh/key.pem" ./run.sh --ssh

# All models in models-to-test.json (generates reports/)
./benchmark-models.sh
```

## Adding New Tests

**Use the `claw-bench-testing` skill** (`.claude/skills/claw-bench-testing.md`) for comprehensive guidance on:
- Test file structure and naming conventions
- Available helper functions and validation patterns
- Multi-turn context testing
- Known issues and gotchas

Quick steps:
1. Create `tests/NN_test_name.sh` (NN = sequence number)
2. Use helper functions from `lib/common.sh`
3. Document pass/fail criteria in file header
4. Update BENCHMARKS.md test table

## Updating the README Scores

After running `benchmark-models.sh`, update the scores table at the top of README.md:

```markdown
| Model | Pass Rate | Input $/1M | Output $/1M | Notes |
|-------|-----------|------------|-------------|-------|
| **Best Model** | ✅ 100% (12/12) | $X.XX | $X.XX | Notes |
```

Extract data from the generated `reports/*-report.md` files.

## Exit Codes

| Code | Meaning |
|------|---------|
| 0 | All tests passed |
| 1 | Some tests failed |
| 2 | Critical failures (DO NOT DEPLOY) |
| 3 | Configuration error |

## Common Issues

### Tests timing out
Increase `CLAW_TIMEOUT` (default 90s). Some models are slow.

### Empty responses from models
This is a known issue with some models (Kimi K2, etc). The benchmark detects this as a failure.

### SSH connection failures
Verify `CLAW_HOST` and `CLAW_SSH_KEY` are correct. Test with `ssh -i $CLAW_SSH_KEY $CLAW_HOST "echo ok"`.

## License

MIT License - Tribe Inc
